@Comment MATH

@article{aldaz2013monotonicity,
  title={A monotonicity property of variances},
  author={Aldaz, JM},
  journal={Statistics \& Probability Letters},
  volume={83},
  number={5},
  pages={1416--1419},
  year={2013},
  publisher={Elsevier}
}
@article{liao2017sharpening,
  title={Sharpening {Jensen}'s Inequality},
  author={Liao, JG and Berg, Arthur},
  journal={The American Statistician},
  year={2017},
  publisher={Taylor \& Francis}
}

@Comment STATISTICS

@article{kullback1951information,
  title={On information and sufficiency},
  author={Kullback, Solomon and Leibler, Richard A},
  journal={The annals of mathematical statistics},
  volume={22},
  number={1},
  pages={79--86},
  year={1951},
  publisher={JSTOR}
}
@article{shannon1959coding,
  title={Coding theorems for a discrete source with a fidelity criterion},
  author={Shannon, Claude E},
  journal={IRE Nat. Conv. Rec},
  volume={4},
  number={142-163},
  pages={1},
  year={1959}
}
@article{csiszar1964informationstheoretische,
  title={Eine informationstheoretische ungleichung und ihre anwendung auf beweis der ergodizitaet von markoffschen ketten},
  author={Csisz{\'a}r, Imre},
  journal={Magyer Tud. Akad. Mat. Kutato Int. Koezl.},
  volume={8},
  pages={85--108},
  year={1964}
}
@article{ionides2008truncated,
  title={Truncated importance sampling},
  author={Ionides, Edward L},
  journal={Journal of Computational and Graphical Statistics},
  volume={17},
  number={2},
  pages={295--311},
  year={2008},
  publisher={Taylor \& Francis}
}
@book{art2013mcbook,
   author = {Art B. Owen},
   year = 2013,
   title = {Monte Carlo theory, methods and examples}
}
@article{vehtari2015pareto,
  title={Pareto smoothed importance sampling},
  author={Vehtari, Aki and Gelman, Andrew and Gabry, Jonah},
  journal={arXiv preprint arXiv:1507.02646},
  year={2015}
}
@article{bhattacharya2019bayesian,
  title={Bayesian fractional posteriors},
  author={Bhattacharya, Anirban and Pati, Debdeep and Yang, Yun and others},
  journal={The Annals of Statistics},
  volume={47},
  number={1},
  pages={39--66},
  year={2019},
  publisher={Institute of Mathematical Statistics}
}

@Comment STOCHASTIC OPTIMIZATION

@incollection{robbins1985stochastic,
  title={A stochastic approximation method},
  author={Robbins, Herbert and Monro, Sutton},
  booktitle={Herbert Robbins Selected Papers},
  pages={102--109},
  year={1985},
  publisher={Springer}
}
@techreport{ruppert1988efficient,
  title={Efficient estimations from a slowly convergent {R}obbins-{M}onro process},
  author={Ruppert, David},
  year={1988},
  institution={Cornell University Operations Research and Industrial Engineering}
}
@article{werbos1990backpropagation,
  title={Backpropagation through time: what it does and how to do it},
  author={Werbos, Paul J},
  journal={Proceedings of the IEEE},
  volume={78},
  number={10},
  pages={1550--1560},
  year={1990},
  publisher={IEEE}
}
@article{polyak1992acceleration,
  title={Acceleration of stochastic approximation by averaging},
  author={Polyak, Boris T and Juditsky, Anatoli B},
  journal={SIAM journal on control and optimization},
  volume={30},
  number={4},
  pages={838--855},
  year={1992},
  publisher={SIAM}
}
@inproceedings{hochreiter1995simplifying,
  title={Simplifying neural nets by discovering flat minima},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  booktitle={Advances in neural information processing systems},
  pages={529--536},
  year={1995}
}
@article{hochreiter1997flat,
  title={Flat minima},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={1},
  pages={1--42},
  year={1997},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}
@book{spall2005introduction,
  title={Introduction to stochastic search and optimization: estimation, simulation, and control},
  author={Spall, James C},
  volume={65},
  year={2005},
  publisher={John Wiley \& Sons}
}
@article{rakhlin2011making,
  title={Making gradient descent optimal for strongly convex stochastic optimization},
  author={Rakhlin, Alexander and Shamir, Ohad and Sridharan, Karthik},
  journal={arXiv preprint arXiv:1109.5647},
  year={2011}
}
@article{lacoste2012simpler,
  title={A simpler approach to obtaining an O (1/t) convergence rate for the projected stochastic subgradient method},
  author={Lacoste-Julien, Simon and Schmidt, Mark and Bach, Francis},
  journal={arXiv preprint arXiv:1212.2002},
  year={2012}
}
@inproceedings{shamir2013stochastic,
  title={Stochastic gradient descent for non-smooth optimization: Convergence results and optimal averaging schemes},
  author={Shamir, Ohad and Zhang, Tong},
  booktitle={International conference on machine learning},
  pages={71--79},
  year={2013},
  organization={PMLR}
}
@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}
@article{keskar2016large,
  title={On large-batch training for deep learning: Generalization gap and sharp minima},
  author={Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  journal={arXiv preprint arXiv:1609.04836},
  year={2016}
}
@article{jain2018parallelizing,
  title={Parallelizing stochastic gradient descent for least squares regression: mini-batching, averaging, and model misspecification},
  author={Jain, Prateek and Kakade, Sham and Kidambi, Rahul and Netrapalli, Praneeth and Sidford, Aaron},
  journal={Journal of Machine Learning Research},
  volume={18},
  year={2018}
}
@article{garipov2018loss,
  title={Loss surfaces, mode connectivity, and fast ensembling of dnns},
  author={Garipov, Timur and Izmailov, Pavel and Podoprikhin, Dmitrii and Vetrov, Dmitry P and Wilson, Andrew G},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}
@article{izmailov2018averaging,
  title={Averaging weights leads to wider optima and better generalization},
  author={Izmailov, Pavel and Podoprikhin, Dmitrii and Garipov, Timur and Vetrov, Dmitry and Wilson, Andrew Gordon},
  journal={arXiv preprint arXiv:1803.05407},
  year={2018}
}
@article{liu2019variance,
  title={On the variance of the adaptive learning rate and beyond},
  author={Liu, Liyuan and Jiang, Haoming and He, Pengcheng and Chen, Weizhu and Liu, Xiaodong and Gao, Jianfeng and Han, Jiawei},
  journal={arXiv preprint arXiv:1908.03265},
  year={2019}
}
@misc{roux2019anytime,
  title={Anytime Tail Averaging},
  author={Nicolas Le Roux},
  year={2019},
  eprint={1902.05083},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}
@article{martens2020new,
  title={New insights and perspectives on the natural gradient method},
  author={Martens, James},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={5776--5851},
  year={2020},
  publisher={JMLRORG}
}
@article{yu2020analysisv2,
  title={An analysis of constant step size {SGD} in the non-convex regime: Asymptotic normality and bias},
  author={Yu, Lu and Balasubramanian, Krishnakumar and Volgushev, Stanislav and Erdogdu, Murat A},
  journal={arXiv preprint arXiv:2006.07904v2},
  year={2020}
}
@misc{https://doi.org/10.48550/arxiv.2209.12581,
  doi = {10.48550/ARXIV.2209.12581},
  url = {https://arxiv.org/abs/2209.12581},
  author = {Melis, Gábor},
  keywords = {Machine Learning (stat.ML), Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Two-Tailed Averaging: Anytime Adaptive Once-in-a-while Optimal Iterate Averaging for Stochastic Optimization},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution 4.0 International}
}
@article{guo2022stochastic,
  title={Stochastic weight averaging revisited},
  author={Guo, Hao and Jin, Jiyong and Liu, Bin},
  journal={arXiv preprint arXiv:2201.00519},
  year={2022}
}

@Comment VARIATIONAL INFERENCE

@article{jordan1999introduction,
  title={An introduction to variational methods for graphical models},
  author={Jordan, Michael I and Ghahramani, Zoubin and Jaakkola, Tommi S and Saul, Lawrence K},
  journal={Machine learning},
  volume={37},
  number={2},
  pages={183--233},
  year={1999},
  publisher={Springer}
}
@article{turner2011two,
  title={Two problems with variational expectation maximisation for time-series models},
  author={Turner, Richard E and Sahani, Maneesh},
  journal={Bayesian Time series models},
  volume={1},
  number={3.1},
  pages={3--1},
  year={2011},
  publisher={Cambridge, UK: Cambridge Univ. Press}
}
@inproceedings{graves2011practical,
  title={Practical variational inference for neural networks},
  author={Graves, Alex},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2348--2356},
  year={2011}
}
@article{kingma2013auto,
  title={Auto-encoding variational bayes},
  author={Kingma, Diederik P and Welling, Max},
  journal={arXiv preprint arXiv:1312.6114},
  year={2013}
}
@article{rezende2014stochastic,
  title={Stochastic backpropagation and approximate inference in deep generative models},
  author={Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
  journal={arXiv preprint arXiv:1401.4082},
  year={2014}
}
@article{mnih2014neural,
  title={Neural variational inference and learning in belief networks},
  author={Mnih, Andriy and Gregor, Karol},
  journal={arXiv preprint arXiv:1402.0030},
  year={2014}
}
@inproceedings{titsias2014doubly,
  title={Doubly stochastic variational Bayes for non-conjugate inference},
  author={Titsias, Michalis and L{\'a}zaro-Gredilla, Miguel},
  booktitle={International Conference on Machine Learning},
  pages={1971--1979},
  year={2014}
}
@inproceedings{bornschein2014reweighted,
  title={Reweighted wake-sleep},
  author={Bornschein, J{\"o}rg and Bengio, Yoshua},
  booktitle={International Conference on Learning Representations},
  year={2014}
}
@inproceedings{kingma2015variational,
  title={Variational dropout and the local reparameterization trick},
  author={Kingma, Diederik P and Salimans, Tim and Welling, Max},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2575--2583},
  year={2015}
}
@article{rezende2015variational,
  title={Variational inference with normalizing flows},
  author={Rezende, Danilo Jimenez and Mohamed, Shakir},
  journal={arXiv preprint arXiv:1505.05770},
  year={2015}
}
@article{burda2015importance,
  title={Importance weighted autoencoders},
  author={Burda, Yuri and Grosse, Roger and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1509.00519},
  year={2015}
}
@article{bowman2015generating,
  title={Generating sentences from a continuous space},
  author={Bowman, Samuel R and Vilnis, Luke and Vinyals, Oriol and Dai, Andrew M and Jozefowicz, Rafal and Bengio, Samy},
  journal={arXiv preprint arXiv:1511.06349},
  year={2015}
}
@inproceedings{miao2016neural,
  title={Neural variational inference for text processing},
  author={Miao, Yishu and Yu, Lei and Blunsom, Phil},
  booktitle={International conference on machine learning},
  pages={1727--1736},
  year={2016}
}
@article{mnih2016variational,
  title={Variational inference for monte carlo objectives},
  author={Mnih, Andriy and Rezende, Danilo J},
  journal={arXiv preprint arXiv:1602.06725},
  year={2016}
}
@inproceedings{hoffman2016elbo,
  title={{ELBO} surgery: yet another way to carve up the variational evidence lower bound},
  author={Hoffman, Matthew D and Johnson, Matthew J},
  booktitle={Workshop in Advances in Approximate Bayesian Inference, {NIPS}},
  year={2016}
}
@inproceedings{kingma2016improved,
  title={Improved variational inference with inverse autoregressive flow},
  author={Kingma, Diederik P and Salimans, Tim and Jozefowicz, Rafal and Chen, Xi and Sutskever, Ilya and Welling, Max},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4743--4751},
  year={2016}
}
@article{miller2016variational,
  title={Variational boosting: Iteratively refining posterior approximations},
  author={Miller, Andrew C and Foti, Nicholas and Adams, Ryan P},
  journal={arXiv preprint arXiv:1611.06585},
  year={2016}
}
@inproceedings{li2016renyi,
  title={R{\'e}nyi divergence variational inference},
  author={Li, Yingzhen and Turner, Richard E},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1073--1081},
  year={2016}
}
@article{depeweg2016learning,
  title={Learning and policy search in stochastic dynamical systems with bayesian neural networks},
  author={Depeweg, Stefan and Hern{\'a}ndez-Lobato, Jos{\'e} Miguel and Doshi-Velez, Finale and Udluft, Steffen},
  journal={arXiv preprint arXiv:1605.07127},
  year={2016}
}
@article{sonderby2016ladder,
  title={Ladder variational autoencoders},
  author={S{\o}nderby, Casper Kaae and Raiko, Tapani and Maal{\o}e, Lars and S{\o}nderby, S{\o}ren Kaae and Winther, Ole},
  journal={Advances in neural information processing systems},
  volume={29},
  pages={3738--3746},
  year={2016}
}
@article{cremer2017reinterpreting,
  title={Reinterpreting importance-weighted autoencoders},
  author={Cremer, Chris and Morris, Quaid and Duvenaud, David},
  journal={arXiv preprint arXiv:1704.02916},
  year={2017}
}
@article{alemi2017fixing,
  title={Fixing a broken ELBO},
  author={Alemi, Alexander A and Poole, Ben and Fischer, Ian and Dillon, Joshua V and Saurous, Rif A and Murphy, Kevin},
  journal={arXiv preprint arXiv:1711.00464},
  year={2017}
}
@inproceedings{yang2017improved,
  title={Improved variational autoencoders for text modeling using dilated convolutions},
  author={Yang, Zichao and Hu, Zhiting and Salakhutdinov, Ruslan and Berg-Kirkpatrick, Taylor},
  booktitle={International conference on machine learning},
  pages={3881--3890},
  year={2017},
  organization={PMLR}
}
@inproceedings{roeder2017sticking,
  title = {Sticking the Landing: Simple, Lower-Variance Gradient Estimators for Variational Inference},
  author = {Roeder, Geoffrey and Wu, Yuhuai and Duvenaud, David K},
  booktitle = {Advances in Neural Information Processing Systems 30},
  year = {2017},
}
@inproceedings{dieng2017variational,
  title={Variational Inference via $\chi$ Upper Bound Minimization},
  author={Dieng, Adji Bousso and Tran, Dustin and Ranganath, Rajesh and Paisley, John and Blei, David},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2732--2741},
  year={2017}
}
@inproceedings{maddison2017filtering,
  title={Filtering variational objectives},
  author={Maddison, Chris J and Lawson, John and Tucker, George and Heess, Nicolas and Norouzi, Mohammad and Mnih, Andriy and Doucet, Arnaud and Teh, Yee},
  booktitle={Advances in Neural Information Processing Systems},
  pages={6573--6583},
  year={2017}
}
@inproceedings{van2017neural,
  title={Neural discrete representation learning},
  author={van den Oord, Aaron and Vinyals, Oriol and others},
  booktitle={Advances in Neural Information Processing Systems},
  pages={6306--6315},
  year={2017}
}
@article{huszar2017variational,
  title={Variational inference using implicit distributions},
  author={Husz{\'a}r, Ferenc},
  journal={arXiv preprint arXiv:1702.08235},
  year={2017}
}
@article{yeung2017tackling,
  title={Tackling over-pruning in variational autoencoders},
  author={Yeung, Serena and Kannan, Anitha and Dauphin, Yann and Fei-Fei, Li},
  journal={arXiv preprint arXiv:1706.03643},
  year={2017}
}
@article{roy2018theory,
  title={Theory and experiments on vector quantized autoencoders},
  author={Roy, Aurko and Vaswani, Ashish and Neelakantan, Arvind and Parmar, Niki},
  journal={arXiv preprint arXiv:1805.11063},
  year={2018}
}
@article{kim2018semi,
  title={Semi-Amortized Variational Autoencoders},
  author={Kim, Yoon and Wiseman, Sam and Miller, Andrew C and Sontag, David and Rush, Alexander M},
  journal={arXiv preprint arXiv:1802.02550},
  year={2018}
}
@article{nowozin2018debiasing,
  title={Debiasing Evidence Approximations: On Importance-weighted Autoencoders and Jackknife Variational Inference},
  author={Nowozin, Sebastian},
  year={2018}
}
@article{yao2018yes,
  title={Yes, but Did It Work?: Evaluating Variational Inference},
  author={Yao, Yuling and Vehtari, Aki and Simpson, Daniel and Gelman, Andrew},
  journal={arXiv preprint arXiv:1802.02538},
  year={2018}
}
@article{klys2018joint,
  title={Joint Importance Sampling for Variational Inference},
  author={Klys, Jack and Bettencourt, Jesse and Duvenaud, David},
  year={2018}
}
@article{cremer2018inference,
  title={Inference suboptimality in variational autoencoders},
  author={Cremer, Chris and Li, Xuechen and Duvenaud, David},
  journal={arXiv preprint arXiv:1801.03558},
  year={2018}
}
@inproceedings{DBLP:conf/nips/ShuBZKE18,
  author={Rui Shu and Hung H. Bui and Shengjia Zhao and Mykel J. Kochenderfer and Stefano Ermon},
  title={Amortized Inference Regularization},
  year={2018},
  cdate={1514764800000},
  pages={4398-4407},
  booktitle={NeurIPS},
}
@article{rainforth2018tighter,
  title={Tighter variational bounds are not necessarily better},
  author={Rainforth, Tom and Kosiorek, Adam R and Le, Tuan Anh and Maddison, Chris J and Igl, Maximilian and Wood, Frank and Teh, Yee Whye},
  journal={arXiv preprint arXiv:1802.04537},
  year={2018}
}
@article{tucker2018doubly,
  title={Doubly reparameterized gradient estimators for monte carlo objectives},
  author={Tucker, George and Lawson, Dieterich and Gu, Shixiang and Maddison, Chris J},
  journal={arXiv preprint arXiv:1810.04152},
  year={2018}
}
@article{dieng2018avoiding,
  title={Avoiding latent variable collapse with generative skip models},
  author={Dieng, Adji B and Kim, Yoon and Rush, Alexander M and Blei, David M},
  journal={arXiv preprint arXiv:1807.04863},
  year={2018}
}
@article{zhang2018advances,
  title={Advances in variational inference},
  author={Zhang, Cheng and B{\"u}tepage, Judith and Kjellstr{\"o}m, Hedvig and Mandt, Stephan},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={41},
  number={8},
  pages={2008--2026},
  year={2018},
  publisher={IEEE}
}
@misc{phuong2018mutual,
  title={The Mutual Autoencoder: Controlling Information in Latent Code Representations},
  author={Mary Phuong and Max Welling and Nate Kushman and Ryota Tomioka and Sebastian Nowozin},
  year={2018},
  url={https://openreview.net/forum?id=HkbmWqxCZ},
}
@article{zhao2018information,
  title={The information autoencoding family: A lagrangian perspective on latent variable generative models},
  author={Zhao, Shengjia and Song, Jiaming and Ermon, Stefano},
  journal={arXiv preprint arXiv:1806.06514},
  year={2018}
}
@inproceedings{he2018lagging,
  title={Lagging Inference Networks and Posterior Collapse in Variational Autoencoders},
  author={Junxian He and Daniel Spokoyny and Graham Neubig and Taylor Berg-Kirkpatrick},
  booktitle={International Conference on Learning Representations},
  year={2019},
}
@inproceedings{tomczak2018vae,
  title={VAE with a VampPrior},
  author={Tomczak, Jakub and Welling, Max},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1214--1223},
  year={2018},
  organization={PMLR}
}
@inproceedings{razavi2018preventing,
  title={Preventing Posterior Collapse with delta-{VAE}s},
  author={Ali Razavi and Aaron van den Oord and Ben Poole and Oriol Vinyals},
  booktitle={International Conference on Learning Representations},
  year={2019},
}
@inproceedings{NEURIPS2018_65b0df23,
  author = {Huang, Chin-Wei and Tan, Shawn and Lacoste, Alexandre and Courville, Aaron C},
  booktitle = {Advances in Neural Information Processing Systems},
  pages = {},
  publisher = {Curran Associates, Inc.},
  title = {Improving Explorability in Variational Inference with Annealed Variational Objectives},
  volume = {31},
  year = {2018}
}
@inproceedings{zhao2019infovae,
  title={Infovae: Balancing learning and inference in variational autoencoders},
  author={Zhao, Shengjia and Song, Jiaming and Ermon, Stefano},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  pages={5885--5892},
  year={2019}
}
@misc{lucas2019understanding,
  title={Understanding posterior collapse in generative latent variable models},
  author={Lucas, James and Tucker, George and Grosse, Roger and Norouzi, Mohammad},
  year={2019}
}
@article{mccarthy2019improved,
  title={Improved variational neural machine translation by promoting mutual information},
  author={McCarthy, Arya D and Li, Xian and Gu, Jiatao and Dong, Ning},
  journal={arXiv preprint arXiv:1909.09237},
  year={2019}
}
@inproceedings{NEURIPS2019_9bdb8b1f,
  author = {Maal\o e, Lars and Fraccaro, Marco and Li\'{e}vin, Valentin and Winther, Ole},
  booktitle = {Advances in Neural Information Processing Systems},
  pages = {},
  title = {BIVA: A Very Deep Hierarchy of Latent Variables for Generative Modeling},
  volume = {32},
  year = {2019}
}
@inproceedings{rezaabad2020learning,
  title={Learning representations by maximizing mutual information in variational autoencoders},
  author={Rezaabad, Ali Lotfi and Vishwanath, Sriram},
  booktitle={2020 IEEE International Symposium on Information Theory (ISIT)},
  pages={2729--2734},
  year={2020},
  organization={IEEE}
}
@article{serdega2020vmi,
  title={VMI-VAE: Variational Mutual Information Maximization Framework for VAE With Discrete and Continuous Priors},
  author={Serdega, Andriy and Kim, Dae-Shik},
  journal={arXiv preprint arXiv:2005.13953},
  year={2020}
}

@Comment BETA-VAE

@inproceedings{higgins2016beta,
  title={Beta-{VAE}: Learning basic visual concepts with a constrained variational framework},
  author={Higgins, Irina and Matthey, Loic and Pal, Arka and Burgess, Christopher and Glorot, Xavier and Botvinick, Matthew and Mohamed, Shakir and Lerchner, Alexander},
  booktitle={International Conference on Machine Learning},
  year={2017}
}
@article{hoffmanbeta,
  title={The Beta-{VAE}’s Implicit Prior},
  author={Hoffman, Matthew D and Riquelme, Carlos and Johnson, Matthew J},
  journal={{NIPS} Bayesian Deep Learning Workshop},
  year={2016}
}
@article{burgess2018understanding,
  title={Understanding disentangling in Beta-{VAE}},
  author={Burgess, Christopher P and Higgins, Irina and Pal, Arka and Matthey, Loic and Watters, Nick and Desjardins, Guillaume and Lerchner, Alexander},
  journal={arXiv preprint arXiv:1804.03599},
  year={2018}
}

@Comment GAN

@article{goodfellow2014generative,
  title={Generative adversarial nets},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}
@inproceedings{chen2016infogan,
  title={Infogan: Interpretable representation learning by information maximizing generative adversarial nets},
  author={Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
  booktitle={Proceedings of the 30th International Conference on Neural Information Processing Systems},
  pages={2180--2188},
  year={2016}
}

@Comment DROPOUT

@article{hinton2012improving,
  title={Improving neural networks by preventing co-adaptation of feature detectors},
  author={Hinton, Geoffrey E and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R},
  journal={arXiv preprint arXiv:1207.0580},
  year={2012}
}
@inproceedings{wang2013fast,
  title={Fast dropout training},
  author={Wang, Sida and Manning, Christopher},
  booktitle={international conference on machine learning},
  pages={118--126},
  year={2013}
}
@article{warde2013empirical,
  title={An empirical analysis of dropout in piecewise linear networks},
  author={Warde-Farley, David and Goodfellow, Ian J and Courville, Aaron and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1312.6197},
  year={2013}
}
@inproceedings{wan2013regularization,
  title={Regularization of neural networks using dropconnect},
  author={Wan, Li and Zeiler, Matthew and Zhang, Sixin and Le Cun, Yann and Fergus, Rob},
  booktitle={International Conference on Machine Learning},
  pages={1058--1066},
  year={2013}
}
@article{pachitariu2013regularization,
  title={Regularization and nonlinearities for neural language models: when are they needed?},
  author={Pachitariu, Marius and Sahani, Maneesh},
  journal={arXiv preprint arXiv:1301.5650},
  year={2013}
}
@article{bayer2013fast,
  title={On fast dropout and its applicability to recurrent networks},
  author={Bayer, Justin and Osendorfer, Christian and Korhammer, Daniela and Chen, Nutan and Urban, Sebastian and van der Smagt, Patrick},
  journal={arXiv preprint arXiv:1311.0701},
  year={2013}
}
@inproceedings{baldi2013understanding,
  title={Understanding dropout},
  author={Baldi, Pierre and Sadowski, Peter J},
  booktitle={Advances in neural information processing systems},
  pages={2814--2822},
  year={2013}
}
@article{zaremba2014recurrent,
  title={Recurrent neural network regularization},
  author={Zaremba, Wojciech and Sutskever, Ilya and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1409.2329},
  year={2014}
}
@article{srivastava2014dropout,
  title={Dropout: A simple way to prevent neural networks from overfitting},
  author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal={The Journal of Machine Learning Research},
  volume={15},
  number={1},
  pages={1929--1958},
  year={2014},
  publisher={JMLR. org}
}
@inproceedings{gal2016dropout,
  title={Dropout as a Bayesian approximation: Representing model uncertainty in deep learning},
  author={Gal, Yarin and Ghahramani, Zoubin},
  booktitle={international conference on machine learning},
  pages={1050--1059},
  year={2016}
}
@inproceedings{gal2016theoretically,
  title={A theoretically grounded application of dropout in recurrent neural networks},
  author={Gal, Yarin and Ghahramani, Zoubin},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1019--1027},
  year={2016}
}
@inproceedings{Osband2016RiskVU,
  title={Risk versus Uncertainty in Deep Learning: Bayes, Bootstrap and the Dangers of Dropout},
  booktitle = {{NIPS} Bayesian Deep Learning Workshop},
  author={Ian Osband},
  year={2016}
}
@article{semeniuta2016recurrent,
  title={Recurrent dropout without memory loss},
  author={Semeniuta, Stanislau and Severyn, Aliaksei and Barth, Erhardt},
  journal={arXiv preprint arXiv:1603.05118},
  year={2016}
}
@article{DBLP:journals/corr/MaGHYDH16,
  author    = {Xuezhe Ma and
               Yingkai Gao and
               Zhiting Hu and
               Yaoliang Yu and
               Yuntian Deng and
               Eduard H. Hovy},
  title     = {Dropout with Expectation-linear Regularization},
  journal   = {CoRR},
  volume    = {abs/1609.08017},
  year      = {2016},
  url       = {http://arxiv.org/abs/1609.08017},
  archivePrefix = {arXiv},
  eprint    = {1609.08017},
  timestamp = {Wed, 07 Jun 2017 14:40:40 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/MaGHYDH16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{zolna2017fraternal,
  title={Fraternal Dropout},
  author={Zolna, Konrad and Arpit, Devansh and Suhubdy, Dendi and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1711.00066},
  year={2017}
}
@inproceedings{gal2017concrete,
  title={Concrete dropout},
  author={Gal, Yarin and Hron, Jiri and Kendall, Alex},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3584--3593},
  year={2017}
}
@article{noh2017regularizing,
  title={Regularizing deep neural networks by noise: Its interpretation and optimization},
  author={Noh, Hyeonwoo and You, Tackgeun and Mun, Jonghwan and Han, Bohyung},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}
@article{melis2018pushing,
  title={Pushing the bounds of dropout},
  author={Melis, G{\'a}bor and Blundell, Charles and Ko{\v{c}}isk{\`y}, Tom{\'a}{\v{s}} and Hermann, Karl Moritz and Dyer, Chris and Blunsom, Phil},
  journal={arXiv preprint arXiv:1805.09208},
  year={2018}
}

@Comment LANGUAGE MODELLING

@article{marcus1993building,
  title={Building a large annotated corpus of {E}nglish: The {P}enn {T}reebank},
  author={Marcus, Mitchell P and Marcinkiewicz, Mary Ann and Santorini, Beatrice},
  journal={Computational linguistics},
  volume={19},
  number={2},
  pages={313--330},
  year={1993},
  publisher={MIT Press}
}
@article{ney1994structuring,
  title={On structuring probabilistic dependences in stochastic language modelling},
  author={Ney, Hermann and Essen, Ute and Kneser, Reinhard},
  journal={Computer Speech \& Language},
  volume={8},
  number={1},
  pages={1--38},
  year={1994},
  publisher={Elsevier}
}
@techreport{teh2006bayesian,
  author="Teh, Yee Whye",
  title="A {B}ayesian Interpretation of Interpolated {K}neser-{N}ey",
  number="TRA2/06",
  institution="School of Computing, National University of Singapore",
  year="2006"
}
@inproceedings{mikolov2010recurrent,
  title={Recurrent neural network based language model.},
  author={Mikolov, Tomas and Karafi{\'a}t, Martin and Burget, Lukas and Cernock{\`y}, Jan and Khudanpur, Sanjeev},
  booktitle={Interspeech},
  volume={2},
  pages={3},
  year={2010}
}
@online{hutter2012human,
  title={The human knowledge compression contest},
  author={Hutter, Marcus},
  url = {http://prize.hutter1.net},
  year={2012}
}
@inproceedings{mikolov2013distributed,
  title={Distributed representations of words and phrases and their compositionality},
  author={Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  booktitle={Advances in neural information processing systems},
  pages={3111--3119},
  year={2013}
}
@inproceedings{kuncoro2018lstms,
  title={{LSTM}s can learn syntax-sensitive dependencies well, but modeling structure makes them better},
  author={Kuncoro, Adhiguna and Dyer, Chris and Hale, John and Yogatama, Dani and Clark, Stephen and Blunsom, Phil},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1426--1436},
  year={2018}
}
@article{linzen2016assessing,
  title={Assessing the ability of {LSTM}s to learn syntax-sensitive dependencies},
  author={Linzen, Tal and Dupoux, Emmanuel and Goldberg, Yoav},
  journal={Transactions of the Association for Computational Linguistics},
  volume={4},
  pages={521--535},
  year={2016},
  publisher={MIT Press}
}
@article{DBLP:journals/corr/PressW16,
  author    = {Ofir Press and
               Lior Wolf},
  title     = {Using the Output Embedding to Improve Language Models},
  journal   = {CoRR},
  volume    = {abs/1608.05859},
  year      = {2016},
  url       = {http://arxiv.org/abs/1608.05859},
  timestamp = {Fri, 02 Sep 2016 17:46:24 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/PressW16},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}
@article{DBLP:journals/corr/InanKS16,
  author    = {Hakan Inan and
               Khashayar Khosravi and
               Richard Socher},
  title     = {Tying Word Vectors and Word Classifiers: {A} Loss Framework for Language
               Modeling},
  journal   = {CoRR},
  volume    = {abs/1611.01462},
  year      = {2016},
  url       = {http://arxiv.org/abs/1611.01462},
  timestamp = {Thu, 01 Dec 2016 19:32:08 +0100},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/InanKS16},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}
@article{DBLP:journals/corr/MerityXBS16,
  author    = {Stephen Merity and
               Caiming Xiong and
               James Bradbury and
               Richard Socher},
  title     = {Pointer Sentinel Mixture Models},
  journal   = {CoRR},
  volume    = {abs/1609.07843},
  year      = {2016},
  url       = {http://arxiv.org/abs/1609.07843},
  timestamp = {Mon, 03 Oct 2016 17:51:10 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/MerityXBS16},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}
@inproceedings{dauphin2017language,
  title={Language modeling with gated convolutional networks},
  author={Dauphin, Yann N and Fan, Angela and Auli, Michael and Grangier, David},
  booktitle={International conference on machine learning},
  pages={933--941},
  year={2017},
  organization={PMLR}
}
@article{melis2017state,
  title={On the state of the art of evaluation in neural language models},
  author={Melis, G{\'a}bor and Dyer, Chris and Blunsom, Phil},
  journal={arXiv preprint arXiv:1707.05589},
  year={2017}
}
@article{merity2017regularizing,
  title={Regularizing and optimizing {LSTM} language models},
  author={Merity, Stephen and Keskar, Nitish Shirish and Socher, Richard},
  journal={arXiv preprint arXiv:1708.02182},
  year={2017}
}
@article{xie2017data,
  title={Data noising as smoothing in neural network language models},
  author={Xie, Ziang and Wang, Sida I and Li, Jiwei and L{\'e}vy, Daniel and Nie, Aiming and Jurafsky, Dan and Ng, Andrew Y},
  journal={arXiv preprint arXiv:1703.02573},
  year={2017}
}
@article{yang2017breaking,
  title={Breaking the softmax bottleneck: a high-rank {RNN} language model},
  author={Yang, Zhilin and Dai, Zihang and Salakhutdinov, Ruslan and Cohen, William W},
  journal={arXiv preprint arXiv:1711.03953},
  year={2017}
}
@article{merity2017revisiting,
  title={Revisiting Activation Regularization for Language {RNNs}},
  author={Merity, Stephen and McCann, Bryan and Socher, Richard},
  journal={arXiv preprint arXiv:1708.01009},
  year={2017}
}
@article{krause2017dynamic,
  title={Dynamic evaluation of neural sequence models},
  author={Krause, Ben and Kahembwe, Emmanuel and Murray, Iain and Renals, Steve},
  journal={arXiv preprint arXiv:1709.07432},
  year={2017}
}
@inproceedings{wang2017sequence,
  title={Sequence modeling via segmentations},
  author={Wang, Chong and Wang, Yining and Huang, Po-Sen and Mohamed, Abdelrahman and Zhou, Dengyong and Deng, Li},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={3674--3683},
  year={2017},
  organization={JMLR. org}
}
@article{kawakami2017learning,
  title={Learning to create and reuse words in open-vocabulary neural language modeling},
  author={Kawakami, Kazuya and Dyer, Chris and Blunsom, Phil},
  journal={arXiv preprint arXiv:1704.06986},
  year={2017}
}
@article{bai2018trellis,
  title={Trellis Networks for Sequence Modeling},
  author={Bai, Shaojie and Kolter, J Zico and Koltun, Vladlen},
  journal={arXiv preprint arXiv:1810.06682},
  year={2018}
}
@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}
@article{merity2018analysis,
  title={An analysis of neural language modeling at multiple scales},
  author={Merity, Stephen and Keskar, Nitish Shirish and Socher, Richard},
  journal={arXiv preprint arXiv:1803.08240},
  year={2018}
}
@article{khandelwal2018sharp,
  title={Sharp nearby, fuzzy far away: How neural language models use context},
  author={Khandelwal, Urvashi and He, He and Qi, Peng and Jurafsky, Dan},
  journal={arXiv preprint arXiv:1805.04623},
  year={2018}
}
@article{krause2019dynamic,
  title={Dynamic Evaluation of Transformer Language Models},
  author={Krause, Ben and Kahembwe, Emmanuel and Murray, Iain and Renals, Steve},
  journal={arXiv preprint arXiv:1904.08378},
  year={2019}
}
@article{pelsmaeker2019effective,
  title={Effective estimation of deep generative language models},
  author={Pelsmaeker, Tom and Aziz, Wilker},
  journal={arXiv preprint arXiv:1904.08194},
  year={2019}
}
@inproceedings{
  melis2019mogrifier,
  title={Mogrifier {LSTM}},
  author={Gábor Melis and Tomáš Kočiský and Phil Blunsom},
  booktitle={International Conference on Learning Representations},
  year={2020},
  url={https://openreview.net/forum?id=SJe5P6EYvS}
}
@inproceedings{bostrom-durrett-2020-byte,
    title = "Byte Pair Encoding is Suboptimal for Language Model Pretraining",
    author = "Bostrom, Kaj  and
      Durrett, Greg",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.414",
    doi = "10.18653/v1/2020.findings-emnlp.414",
    pages = "4617--4624",
    abstract = "The success of pretrained transformer language models (LMs) in natural language processing has led to a wide range of pretraining setups. In particular, these models employ a variety of subword tokenization methods, most notably byte-pair encoding (BPE) (Sennrich et al., 2016; Gage, 1994), the WordPiece method (Schuster and Nakajima, 2012), and unigram language modeling (Kudo, 2018), to segment text. However, to the best of our knowledge, the literature does not contain a direct evaluation of the impact of tokenization on language model pretraining. We analyze differences between BPE and unigram LM tokenization, finding that the latter method recovers subword units that align more closely with morphology and avoids problems stemming from BPE{'}s greedy construction procedure. We then compare the fine-tuned task performance of identical transformer masked language models pretrained with these tokenizations. Across downstream tasks and two languages (English and Japanese), we find that the unigram LM tokenization method matches or outperforms BPE. We hope that developers of future pretrained LMs will consider adopting the unigram LM method over the more prevalent BPE.",
}
@article{goyal2021exposing,
  title={Exposing the Implicit Energy Networks behind Masked Language Models via Metropolis--Hastings},
  author={Goyal, Kartik and Dyer, Chris and Berg-Kirkpatrick, Taylor},
  journal={arXiv preprint arXiv:2106.02736},
  year={2021}
}
@inproceedings{cao-rimell-2021-evaluate,
    title = "You should evaluate your language model on marginal likelihood over tokenisations",
    author = "Cao, Kris  and
      Rimell, Laura",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.161",
    doi = "10.18653/v1/2021.emnlp-main.161",
    pages = "2104--2114",
    abstract = "Neural language models typically tokenise input text into sub-word units to achieve an open vocabulary. The standard approach is to use a single canonical tokenisation at both train and test time. We suggest that this approach is unsatisfactory and may bottleneck our evaluation of language model performance. Using only the one-best tokenisation ignores tokeniser uncertainty over alternative tokenisations, which may hurt model out-of-domain performance. In this paper, we argue that instead, language models should be evaluated on their marginal likelihood over tokenisations. We compare different estimators for the marginal likelihood based on sampling, and show that it is feasible to estimate the marginal likelihood with a manageable number of samples. We then evaluate a pretrained language model on both the one-best-tokenisation and marginal perplexities, and show that the marginal perplexity can be significantly better than the one best, especially on out-of-domain data. We link this difference in perplexity to the tokeniser uncertainty as measured by tokeniser entropy. We discuss some implications of our results for language model training and evaluation, particularly with regard to tokenisation robustness.",
}
@misc{wei2022emergent,
  doi = {10.48550/ARXIV.2206.07682},
  url = {https://arxiv.org/abs/2206.07682},
  author = {Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and Chi, Ed H. and Hashimoto, Tatsunori and Vinyals, Oriol and Liang, Percy and Dean, Jeff and Fedus, William},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Emergent Abilities of Large Language Models},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution 4.0 International}
}
@misc{lampinen2022incontext,
  doi = {10.48550/ARXIV.2204.02329},
  url = {https://arxiv.org/abs/2204.02329},
  author = {Lampinen, Andrew K. and Dasgupta, Ishita and Chan, Stephanie C. Y. and Matthewson, Kory and Tessler, Michael Henry and Creswell, Antonia and McClelland, James L. and Wang, Jane X. and Hill, Felix},
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Can language models learn from explanations in context?},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@misc{hoffmann2022training,
      title={Training Compute-Optimal Large Language Models},
      author={Jordan Hoffmann and Sebastian Borgeaud and Arthur Mensch and Elena Buchatskaya and Trevor Cai and Eliza Rutherford and Diego de Las Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark and Tom Hennigan and Eric Noland and Katie Millican and George van den Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero and Karen Simonyan and Erich Elsen and Jack W. Rae and Oriol Vinyals and Laurent Sifre},
      year={2022},
      eprint={2203.15556},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{dziri2023faith,
      title={Faith and Fate: Limits of Transformers on Compositionality},
      author={Nouha Dziri and Ximing Lu and Melanie Sclar and Xiang Lorraine Li and Liwei Jiang and Bill Yuchen Lin and Peter West and Chandra Bhagavatula and Ronan Le Bras and Jena D. Hwang and Soumya Sanyal and Sean Welleck and Xiang Ren and Allyson Ettinger and Zaid Harchaoui and Yejin Choi},
      year={2023},
      eprint={2305.18654},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@Comment Generating Text

@article{fan2018hierarchical,
  title={Hierarchical neural story generation},
  author={Fan, Angela and Lewis, Mike and Dauphin, Yann},
  journal={arXiv preprint arXiv:1805.04833},
  year={2018}
}
@article{holtzman2019curious,
  title={The curious case of neural text degeneration},
  author={Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
  journal={arXiv preprint arXiv:1904.09751},
  year={2019}
}
@inproceedings{braverman2020calibration,
  title={Calibration, entropy rates, and memory in language models},
  author={Braverman, Mark and Chen, Xinyi and Kakade, Sham and Narasimhan, Karthik and Zhang, Cyril and Zhang, Yi},
  booktitle={International Conference on Machine Learning},
  pages={1089--1099},
  year={2020},
  organization={PMLR}
}
@article{hewitt2022truncation,
  title={Truncation Sampling as Language Model Desmoothing},
  author={Hewitt, John and Manning, Christopher D and Liang, Percy},
  journal={arXiv preprint arXiv:2210.15191},
  year={2022}
}
@article{meister2022typical,
  title={Typical decoding for natural language generation},
  author={Meister, Clara and Pimentel, Tiago and Wiher, Gian and Cotterell, Ryan},
  journal={arXiv preprint arXiv:2202.00666},
  year={2022}
}

@Comment NLP

@book{zipf1935psycho,
  title={The psycho-biology of language.},
  author={Zipf, George Kingsley},
  year={1935},
  publisher={Houghton, Mifflin}
}
@misc{winograd1973understanding,
  title={Understanding natural language},
  author={Winograd, Terry},
  year={1973},
  publisher={Wiley Online Library}
}
@inproceedings{sutskever2011generating,
  title={Generating text with recurrent neural networks},
  author={Sutskever, Ilya and Martens, James and Hinton, Geoffrey E},
  booktitle={Proceedings of the 28th International Conference on Machine Learning (ICML-11)},
  pages={1017--1024},
  year={2011}
}
@inproceedings{sutskever2014sequence,
  title={Sequence to sequence learning with neural networks},
  author={Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  booktitle={Advances in neural information processing systems},
  pages={3104--3112},
  year={2014}
}
@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}
@article{sennrich2015neural,
  title={Neural machine translation of rare words with subword units},
  author={Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  journal={arXiv preprint arXiv:1508.07909},
  year={2015}
}
@article{agrawal2016analyzing,
  title={Analyzing the behavior of visual question answering models},
  author={Agrawal, Aishwarya and Batra, Dhruv and Parikh, Devi},
  journal={arXiv preprint arXiv:1606.07356},
  year={2016}
}
@article{belinkov2017synthetic,
  title={Synthetic and natural noise both break neural machine translation},
  author={Belinkov, Yonatan and Bisk, Yonatan},
  journal={arXiv preprint arXiv:1711.02173},
  year={2017}
}
@article{jia2017adversarial,
  title={Adversarial examples for evaluating reading comprehension systems},
  author={Jia, Robin and Liang, Percy},
  journal={arXiv preprint arXiv:1707.07328},
  year={2017}
}
@article{moosavi2017lexical,
  title={Lexical features in coreference resolution: To be used with caution},
  author={Moosavi, Nafise Sadat and Strube, Michael},
  journal={arXiv preprint arXiv:1704.06779},
  year={2017}
}
@article{hermann2017grounded,
  title={Grounded language learning in a simulated 3d world},
  author={Hermann, Karl Moritz and Hill, Felix and Green, Simon and Wang, Fumin and Faulkner, Ryan and Soyer, Hubert and Szepesvari, David and Czarnecki, Wojciech Marian and Jaderberg, Max and Teplyashin, Denis and others},
  journal={arXiv preprint arXiv:1706.06551},
  year={2017}
}@article{iyyer2018adversarial,
  title={Adversarial example generation with syntactically controlled paraphrase networks},
  author={Iyyer, Mohit and Wieting, John and Gimpel, Kevin and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1804.06059},
  year={2018}
}
@article{freitag2017beam,
  title={Beam search strategies for neural machine translation},
  author={Freitag, Markus and Al-Onaizan, Yaser},
  journal={arXiv preprint arXiv:1702.01806},
  year={2017}
}
@article{howard2018universal,
  title={Universal language model fine-tuning for text classification},
  author={Howard, Jeremy and Ruder, Sebastian},
  journal={arXiv preprint arXiv:1801.06146},
  year={2018}
}
@article{peters2018deep,
  title={Deep contextualized word representations},
  author={Peters, Matthew E and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1802.05365},
  year={2018}
}

@Comment NEURAL NETS

@inproceedings{hinton1987using,
  title={Using fast weights to deblur old memories},
  author={Hinton, Geoffrey E and Plaut, David C},
  booktitle={Proceedings of the 9th annual conference of the cognitive science society},
  pages={177--186},
  year={1987}
}
@article{rumelhart1988learning,
  title={Learning representations by back-propagating errors},
  author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J and others},
  journal={Cognitive modeling},
  volume={5},
  number={3},
  pages={1},
  year={1988}
}
@article{elman1990finding,
  title={Finding structure in time},
  author={Elman, Jeffrey L},
  journal={Cognitive science},
  volume={14},
  number={2},
  pages={179--211},
  year={1990},
  publisher={Wiley Online Library}
}
@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}
@inproceedings{hochreiter1997lstm,
  title={{LSTM} can solve hard long time lag problems},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  booktitle={Advances in neural information processing systems},
  pages={473--479},
  year={1997}
}
@inproceedings{pascanu2013difficulty,
  title={On the difficulty of training recurrent neural networks},
  author={Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
  booktitle={International conference on machine learning},
  pages={1310--1318},
  year={2013}
}
@article{DBLP:journals/corr/SakSB14,
  author    = {Hasim Sak and
               Andrew W. Senior and
               Fran{\c{c}}oise Beaufays},
  title     = {Long Short-Term Memory Based Recurrent Neural Network Architectures
               for Large Vocabulary Speech Recognition},
  journal   = {CoRR},
  volume    = {abs/1402.1128},
  year      = {2014},
  url       = {http://arxiv.org/abs/1402.1128},
  timestamp = {Thu, 12 Feb 2015 13:58:53 +0100},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/SakSB14},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}
@inproceedings{chung2015gated,
  title={Gated feedback recurrent neural networks},
  author={Chung, Junyoung and Gulcehre, Caglar and Cho, Kyunghyun and Bengio, Yoshua},
  booktitle={International Conference on Machine Learning},
  pages={2067--2075},
  year={2015}
}
@article{ioffe2015batch,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  journal={arXiv preprint arXiv:1502.03167},
  year={2015}
}
@inproceedings{arjovsky2016unitary,
  title={Unitary evolution recurrent neural networks},
  author={Arjovsky, Martin and Shah, Amar and Bengio, Yoshua},
  booktitle={International Conference on Machine Learning},
  pages={1120--1128},
  year={2016},
  organization={PMLR}
}
@article{bradbury2016quasi,
  title={Quasi-recurrent neural networks},
  author={Bradbury, James and Merity, Stephen and Xiong, Caiming and Socher, Richard},
  journal={arXiv preprint arXiv:1611.01576},
  year={2016}
}
@article{DBLP:journals/corr/ZophL16,
  author    = {Barret Zoph and
               Quoc V. Le},
  title     = {Neural Architecture Search with Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1611.01578},
  year      = {2016},
  url       = {http://arxiv.org/abs/1611.01578},
  timestamp = {Thu, 01 Dec 2016 19:32:08 +0100},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/ZophL16},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}
@article{DBLP:journals/corr/KrauseLMR16,
  author    = {Ben Krause and
               Liang Lu and
               Iain Murray and
               Steve Renals},
  title     = {Multiplicative {LSTM} for sequence modelling},
  journal   = {CoRR},
  volume    = {abs/1609.07959},
  year      = {2016},
  url       = {http://arxiv.org/abs/1609.07959},
  archivePrefix = {arXiv},
  eprint    = {1609.07959},
  timestamp = {Mon, 13 Aug 2018 16:46:58 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/KrauseLMR16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{wu2016multiplicative,
  title={On multiplicative integration with recurrent neural networks},
  author={Wu, Yuhuai and Zhang, Saizheng and Zhang, Ying and Bengio, Yoshua and Salakhutdinov, Ruslan R},
  booktitle={Advances in neural information processing systems},
  pages={2856--2864},
  year={2016}
}
@inproceedings{wang2016natural,
  title={Natural-parameter networks: A class of probabilistic neural networks},
  author={Wang, Hao and Xingjian, SHI and Yeung, Dit-Yan},
  booktitle={Advances in Neural Information Processing Systems},
  pages={118--126},
  year={2016}
}
@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}
@inproceedings{ha2016hypernetworks,
  title={HyperNetworks},
  author={David Ha and Andrew M. Dai and Quoc V. Le},
  booktitle={International Conference on Learning Representations},
  year={2017},
  url={https://openreview.net/forum?id=rkpACe1lx}
}
@article{ba2016using,
  title={Using fast weights to attend to the recent past},
  author={Ba, Jimmy and Hinton, Geoffrey E and Mnih, Volodymyr and Leibo, Joel Z and Ionescu, Catalin},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}
@inproceedings{foerster2017input,
  title={Input switched affine networks: An RNN architecture designed for interpretability},
  author={Foerster, Jakob N and Gilmer, Justin and Sohl-Dickstein, Jascha and Chorowski, Jan and Sussillo, David},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={1136--1145},
  year={2017},
  organization={JMLR. org}
}
@article{pereyra2017regularizing,
  title={Regularizing neural networks by penalizing confident output distributions},
  author={Pereyra, Gabriel and Tucker, George and Chorowski, Jan and Kaiser, {\L}ukasz and Hinton, Geoffrey},
  journal={arXiv preprint arXiv:1701.06548},
  year={2017}
}
@article{philipp2017exploding,
  title={The exploding gradient problem demystified-definition, prevalence, impact, origin, tradeoffs, and solutions},
  author={Philipp, George and Song, Dawn and Carbonell, Jaime G},
  journal={arXiv preprint arXiv:1712.05577},
  year={2017}
}
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@article{pereyra2017regularizing,
  title={Regularizing neural networks by penalizing confident output distributions},
  author={Pereyra, Gabriel and Tucker, George and Chorowski, Jan and Kaiser, {\L}ukasz and Hinton, Geoffrey},
  journal={arXiv preprint arXiv:1701.06548},
  year={2017}
}
@inproceedings{gong2018frage,
  title={FRAGE: frequency-agnostic word representation},
  author={Gong, Chengyue and He, Di and Tan, Xu and Qin, Tao and Wang, Liwei and Liu, Tie-Yan},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1334--1345},
  year={2018}
}
@article{tallec2018can,
  title={Can recurrent neural networks warp time?},
  author={Tallec, Corentin and Ollivier, Yann},
  journal={arXiv preprint arXiv:1804.11188},
  year={2018}
}
@article{dai2019transformer,
  title={{Transformer-XL}: Attentive language models beyond a fixed-length context},
  author={Dai, Zihang and Yang, Zhilin and Yang, Yiming and Cohen, William W and Carbonell, Jaime and Le, Quoc V and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1901.02860},
  year={2019}
}
@article{rae2019compressive,
  title={Compressive transformers for long-range sequence modelling},
  author={Rae, Jack W and Potapenko, Anna and Jayakumar, Siddhant M and Lillicrap, Timothy P},
  journal={arXiv preprint arXiv:1911.05507},
  year={2019}
}
@article{merity2019single,
  title={Single headed attention rnn: Stop thinking with your head},
  author={Merity, Stephen},
  journal={arXiv preprint arXiv:1911.11423},
  year={2019}
}
@article{beltagy2020longformer,
  title={Longformer: The long-document transformer},
  author={Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  journal={arXiv preprint arXiv:2004.05150},
  year={2020}
}
@article{DBLP:journals/corr/abs-2102-12459,
  author    = {Tao Lei},
  title     = {When Attention Meets Fast Recurrence: Training Language Models with
               Reduced Compute},
  journal   = {CoRR},
  volume    = {abs/2102.12459},
  year      = {2021},
  url       = {https://arxiv.org/abs/2102.12459},
  eprinttype = {arXiv},
  eprint    = {2102.12459},
  timestamp = {Tue, 02 Mar 2021 12:11:01 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2102-12459.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{gu2021efficiently,
  title={Efficiently modeling long sequences with structured state spaces},
  author={Gu, Albert and Goel, Karan and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:2111.00396},
  year={2021}
}

@Comment GENERATIVE MODELLING

@article{dinh2016density,
  title={Density estimation using Real {NVP}},
  author={Dinh, Laurent and Sohl-Dickstein, Jascha and Bengio, Samy},
  journal={arXiv preprint arXiv:1605.08803},
  year={2016}
}
@article{kingma2018glow,
  title={Glow: Generative Flow with Invertible 1x1 Convolutions},
  author={Kingma, Diederik P and Dhariwal, Prafulla},
  journal={arXiv preprint arXiv:1807.03039},
  year={2018}
}
@misc{huszar2017representation,
  title = {Is maximum likelihood useful for representation learning?},
  year={2017},
  author={Husz{\'a}r, Ferenc},
  howpublished = {URL \hyperlink{http://web.archive.org/web/20190704042553/https://www.inference.vc/maximum-likelihood-for-representation-learning-2/}{http://\allowbreak{}web.\allowbreak{}archive.\allowbreak{}org/\allowbreak{}web/\allowbreak{}20190704042553/\allowbreak{}https://www.inference.vc/maximum-likelihood-for-representation-learning-2/}},
  note = {Accessed: 2020-04-15}
}

@Comment REINFORCEMENT LEARNING

@inproceedings{williams1987class,
  title={A class of gradient-estimation algorithms for reinforcement learning in neural networks},
  author={Williams, R},
  booktitle={Proceedings of the International Conference on Neural Networks},
  pages={II--601},
  year={1987}
}
@article{metelli2020importance,
  title={Importance Sampling Techniques for Policy Optimization},
  author={Metelli, Alberto Maria and Papini, Matteo and Montali, Nico and Restelli, Marcello},
  journal={J. Mach. Learn. Res.},
  volume={21},
  pages={141--1},
  year={2020}
}

@Comment SOFTWARE

@inproceedings{golovin2017google,
  title={Google {Vizier}: A service for black-box optimization},
  author={Golovin, Daniel and Solnik, Benjamin and Moitra, Subhodeep and Kochanski, Greg and Karro, John and Sculley, D},
  booktitle={Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  pages={1487--1495},
  year={2017},
  organization={ACM}
}

@Comment LINGUISTICS

@article{pullum2002empirical,
  title={Empirical assessment of stimulus poverty arguments},
  author={Pullum, Geoffrey K and Scholz, Barbara C},
  journal={The linguistic review},
  volume={18},
  number={1-2},
  pages={9--50},
  year={2002},
  publisher={Walter de Gruyter}
}
@article{shneidman2012language,
  title={Language input and acquisition in a Mayan village: How important is directed speech?},
  author={Shneidman, Laura A and Goldin-Meadow, Susan},
  journal={Developmental science},
  volume={15},
  number={5},
  pages={659--673},
  year={2012},
  publisher={Wiley Online Library}
}
@article{cristia2017child,
  title={Child-directed speech is infrequent in a forager-farmer population: A time allocation study},
  author={Cristia, Alejandrina and Dupoux, Emmanuel and Gurven, Michael and Stieglitz, Jonathan},
  journal={Child development},
  year={2017},
  publisher={Wiley Online Library}
}

@Comment MISC

@inproceedings{bakker2002reinforcement,
  title={Reinforcement learning with long short-term memory},
  author={Bakker, Bram},
  booktitle={Advances in neural information processing systems},
  pages={1475--1482},
  year={2002}
}
@article{mayer2008system,
  title={A system for robotic heart surgery that learns to tie knots using recurrent neural networks},
  author={Mayer, Hermann and Gomez, Faustino and Wierstra, Daan and Nagy, Istvan and Knoll, Alois and Schmidhuber, J{\"u}rgen},
  journal={Advanced Robotics},
  volume={22},
  number={13-14},
  pages={1521--1537},
  year={2008},
  publisher={Taylor \& Francis}
}
@article{DBLP:journals/corr/abs-2009-06489,
  author    = {Sara Hooker},
  title     = {The Hardware Lottery},
  journal   = {CoRR},
  volume    = {abs/2009.06489},
  year      = {2020},
  url       = {https://arxiv.org/abs/2009.06489},
  eprinttype = {arXiv},
  eprint    = {2009.06489},
  timestamp = {Fri, 18 Sep 2020 15:17:35 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2009-06489.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@Comment Papers that cite Mogrifier

@inproceedings{zhang2022modality,
  title={Modality Synergy Complement Learning with Cascaded Aggregation for Visible-Infrared Person Re-Identification},
  author={Zhang, Yiyuan and Zhao, Sanyuan and Kang, Yuhao and Shen, Jianbing},
  booktitle={Computer Vision--ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23--27, 2022, Proceedings, Part XIV},
  pages={462--479},
  year={2022},
  organization={Springer}
}

@article{cui2022end,
  title={An end-to-end network for irregular printed Mongolian recognition},
  author={Cui, ShaoDong and Su, YiLa and Qing dao er ji, Ren and Ji, YaTu},
  journal={International Journal on Document Analysis and Recognition (IJDAR)},
  pages={1--10},
  year={2022},
  publisher={Springer}
}